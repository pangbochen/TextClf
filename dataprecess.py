# -*- coding: UTF-8 -*-

import os
import numpy as np
import string
from collections import Counter
import pandas as pd
from tqdm import tqdm
import random
import time
# from utils import log_time_delta
from dataloader import Dataset
import torch
from torch.autograd import Variable
from codecs import open
import pickle
from functools import wraps


# 定义字典
# inherite from python class dcit
class Alphabet(dict):
    def __init__(self, start_feature_id=1, alphabet_type='text'):
        super(Alphabet, self).__init__()
        self.fid = start_feature_id
        if alphabet_type == 'text':
            init_words = ['[PADDING]', '[UNK]', '[END]']
            self.add_all(init_words)
            self.padding_token = self.get('[PADDING]')
            self.unknown_token = self.get('[UNK]')
            self.end_token = self.get('[END]')

    # add new token into the dictionary
    # token : idx
    # idx generated by self.fid
    def add(self, item):
        idx = self.get(item, None)
        if idx is None:
            idx = self.fid
            self.fid += 1  # update idx
            self[item] = idx
        return idx

    # add a list of token
    def add_all(self, words):
        for word in words:
            self.add(word)

    # dump alphabet
    def dump(self, fname, path='temp'):
        if not os.path.exists(path):
            os.mkdir(path)
        with open(os.path.join(path, fname), 'w', encoding='utf-8') as f:
            for k in sorted(self.keys()):
                f.write('{}\t{}\n'.format(k, self[k]))

class DottableDict(dict):
    def __init__(self, *args, **kwargs):
        dict.__init__(self, *args, **kwargs)
        self.__dict__ = self
        self.allow_dotting()

    def allow_dotting(self, state=True):
        if state:
            self.__dict__ = self
        else:
            self.__dict__ = dict()


# same as bucketiterator in torchtext
class BucketIterator(object):
    def __init__(self, data, opt=None, batch_size=2, shuffle=True, test=False, position=False):
        self.shuffle = shuffle
        self.data = data
        self.batch_size = batch_size
        self.test = test
        if opt is not None:
            self.init_opt(opt)

    # opt具有最高的优先级
    def init_opt(self, opt):
        self.batch_size = opt.batch_size
        self.shuffle = opt.__dict__.get('shuffle', self.shuffle)
        self.position = opt.__dict__.get('position', False)
        self.padding_token = opt.alphabet.padding_token

    # pytorch tensor transform
    # for further use, it is not necessary to consider cuda
    def transform(self, data, opt):
        data = data.reset_index()
        text = torch.Tensor(data.text)
        label = torch.Tensor(data.label.tolist())
        # position
        if self.position:
            position_tensor = self.get_position(data.text)
            return DottableDict({
                'text' : (text, position_tensor),
                'label' : label
            })
        else:
            return DottableDict({
                'text' : text,
                'label' : label
            })

    # generate position
    def get_position(self, data):
        position = np.array([[pos_i+1 if w_i != self.padding_token else 0 for pos_i, w_i in enumerate(inst)] for inst in data])
        position_tensor = Variable(torch.Tensor(position), volatile=self.test)
        return position_tensor

    # define the __iter__ for this iterator
    # key part for this iterator
    def __iter__(self):
        if self.shuffle:
            self.data = self.data.sample(frac=1).reset_index(drop=True)
        batch_nums = int(len(self.data)/self.batch_size)
        for i in range(batch_nums):
            yield self.transform(self.data[i*self.batch_size:(i+1)*self.batch_size])
        # handle for the last part
        yield self.transform(self.data[-1*self.batch_size:])


# util function log time for this function
def log_time(func):
    @wraps(func)
    def _deco(*args, **kwargs):
        start_time = time.time()
        ret = func(*args, **kwargs)
        end_time = time.time()
        delta = end_time - start_time
        print('{} run {.2f} senconds'.format(func.__name__, delta))
        return ret
    return _deco

# vector
# after load raw data (label, text(int word list format) ) generate alphabet
def getEmbeddingFile(opt):
    # two kind: glove and w2v
    embedding_name = opt.__dict__.get('embedding', 'glove_6b_300')
    if embedding_name.startswith('glove'):
        return os.path.join('.vector_cache', 'glove.6B.300d.txt')
    else:
        return opt.embedding_dir

# description of the glove txt, for each line, word and vector with size of embedding_size
@log_time
def load_text_vec(alphabet, filename='', embedding_size=-1):
    vectors = {}
    with open(filename, encoding='utf-8') as f:
        for line in tqdm(f):
            items = line.strip().split(' ') # generate the word_vector from this line
            if len(items) == 2:
                vocab_size, embedding_size = items[0], items[1]
            else:
                word = items[0]
                if word in alphabet:
                    vectors[word] = items[1:]
    print('embedding_size is {}'.format(embedding_size))
    print('vocab_size is {}'.format(vocab_size))
    print('words needs to be found in the alphabet {}'.format(len(alphabet)))
    print('words found in the embedding {}'.format(len(vectors.keys())))
    # update embedding_size
    if embedding_size == -1:
        embedding_size = len(vectors[list(vectors.keys())[0]])
    return vectors, embedding_size

@log_time
def vectors_lookup(vectors, vocab, dim):
    '''
    :param vectors: vectors generated by getSubVectors, dict {word: word_vector}
    :param vocab: class Alphabet object
    :param dim: embedding_size
    :return: vectors
    '''
    embedding = np.zeros((len(vocab), dim)) # embeddings for the vocabulary
    count = 1
    for word in vocab:
        # update each word with embedding of the vocabulary
        if word in vectors:
            count += 1
            embedding[vocab[word]] = np.random.uniform(-0.5, +0.5, dim) # random setting for the unknown word
    print('{} word in embedding.'.format(count))
    return embedding

@log_time
def getSubVectors(opt, alphabet):
    output_FN = 'tmp/{}.vec'.format(opt.dataset)
    # check
    if not os.path.exists(output_FN) or opt.debug:
        # load embedding file, the glove model file
        # get the filenaem of glove model
        glove_file = getEmbeddingFile(opt)
        wordset = set(alphabet.keys())
        # load text vector
        loaded_vectors, embedding_size = load_text_vec(wordset, glove_file)
        # about loaded_vectors, dict { word: word_vector }
        vectors = vectors_lookup(loaded_vectors, alphabet, embedding_size)
        # vectors is the np.array, embedding for words in vocab
        # then handle the unknow part
        if opt.debug:
            if not os.path.exists('temp'):
                os.mkdir('temp')
            with open('temp/oov.txt', 'w', 'utf-8') as f:
                unknow_set = set(alphabet.keys()) - set(loaded_vectors.keys())
                f.write('\n'.join(unknow_set))
        if opt.debug:
            pickle.dump(vectors, open(output_FN, 'wb'))
        return vectors
    else:
        print('load cache')
    return pickle.load(open(output_FN, 'rb'))

# main part for load data
'''

'''
# get Dataset: load the dataset class object
def getDataset(opt):
    # use dataloader part
    import dataloader
    dataset = dataloader.getDataset(opt)
    # return the processed file name: text and label
    # by dataset.process() funtion
    return dataset.getFormatedData()

# get clean data
import re
def clean(text):
    '''
    :param text: str, raw text
    :return: cleaned of list of token
    '''
    sub_list = ['<br/>', '<br>', '<br']
    for token in sub_list:
        text = re.sub(token, ' ', text)
    # sub labels
    text = re.sub("[\s+\.\!\/_,$%^*()\(\)<>+\"\[\]\-\?;:\'{}`]+|[+——！，。？、~@#￥%……&*（）]+", " ", text)
    return text.lower().split()

@log_time
def get_clean_data(opt):
    # ouput_FN
    output_FN = 'tmp/{}.data'.format(opt.dataset)
    # check for have prossed
    if not os.path.exists(output_FN):
        datas = []
        # processed file names
        # train and test two csv file for and from pandas        for filename in getDataset(opt):
        for filename in getDataset(opt):
            df = pd.read_csv(filename, header=None, sep='\t', names=['text', 'label']).fillna('0')
            df['text'] = df['text'].apply(clean) # apply clean function: raw str -> cleaned list of token
            datas.append(df)
        return datas
    else:
        print('load cache data')
        return pickle.load(open(output_FN, 'rb'))

def loadDataWithoutEmbedding(opt):
    dataset = []
    for filename in getDataset(opt):
        df = pd.read_csv(filename, header=None, sep='\t', names=['text', 'label'])
        df['text'] = df['text'].str.lower()
        dataset.append((df['text'], df['label']))
    return dataset

def loadData(opt, embedding=True):
    if embedding == False:
        return loadDataWithoutEmbedding(opt)
    # load data
    datas = get_clean_data(opt) # text: list of cleaned token ; label : int 0,1
    # alphabet
    alphabet = Alphabet(start_feature_id=0)
    label_alphabet = Alphabet(start_feature_id=0, alphabet_type='label')

    df = pd.concat(datas)
    # save for the dem
    df.to_csv('demo.txt', sep='\t', index=False)

    # then get label and word for text
    label_set = set(df['label'])
    label_alphabet.add_all(label_set)

    word_set = set()
    for w_l in df['text']:
        if w_l is not None:
            for word in w_l:
                word_set.add(word)
    alphabet.add_all(word_set)

    # get vectors
    # vector is np.array, (vocab_size, embedding_dim)
    vectors = getSubVectors(opt, alphabet)

    # pad for max_seq_len
    if opt.max_seq_len == -1:
        # find the max len among the data dataframe
        opt.max_seq_len  = df.apply(lambda row: row['text'].__len__(), axis=1).max()

    # update opt
    opt.vocab_size = len(alphabet)
    opt.label_size = len(label_alphabet)
    opt.embedding_dim = vectors.shape[-1]
    opt.alphabet = alphabet
    # update idx into max_seq_len
    for data in datas:
        data['text'] = data['text'].apply(lambda text: [alphabet.get(word, alphabet.unknown_token) for word in text[:opt.max_seq_len]]+[alphabet.padding_token]*int(opt.max_seq_len-len(text)))
        data['label'] = data['label'].apply(lambda text: label_alphabet.get(text))

    return map(lambda x : BucketIterator(x, opt), datas)

